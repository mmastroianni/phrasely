{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ðŸ§­ Phrasely Evaluation & Visualization Notebook\n",
    "### This notebook summarizes clustering quality, structure, and representative examples for the Phrasely pipeline (embeddings â†’ SVD â†’ UMAP â†’ HDBSCAN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Phase 1: Loading data, embedding it, reducing it, generating clusters, all in GPU (if you have it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import dump, load\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from phrasely.data_loading.csv_loader import CSVLoader\n",
    "from phrasely.embeddings.phrase_embedder import PhraseEmbedder\n",
    "from phrasely.reduction.svd_reducer import SVDReducer\n",
    "from phrasely.clustering.hdbscan_clusterer import HDBSCANClusterer\n",
    "from phrasely.medoids.medoid_selector import MedoidSelector\n",
    "from phrasely.evaluation import ClusterEvaluator\n",
    "from phrasely.reduction.visualization_reducer import UMAPReducer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(input_path=\"../data/msmarco.csv\")\n",
    "phrases = loader.load()\n",
    "print(f\"Loaded {len(phrases)} phrases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phrasely.embeddings.phrase_embedder import PhraseEmbedder\n",
    "\n",
    "embedder = PhraseEmbedder(batch_size=8)\n",
    "embeddings = embedder.embed(phrases, dataset_name=\"msmarco_full\")\n",
    "print(\"Embeddings:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = np.random.choice(len(embeddings), size=100_000, replace=False)\n",
    "embeddings_sample = embeddings[sample_idx]\n",
    "phrases_sample = [phrases[i] for i in sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phrasely.reduction.two_stage_reducer import TwoStageReducer\n",
    "\n",
    "reducer = TwoStageReducer(\n",
    "    svd_components=256,\n",
    "    umap_components=10,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "reduced_two_stage = reducer.reduce(embeddings_sample)\n",
    "print(f\"Output shape: {reduced_two_stage.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phrasely.clustering.hdbscan_clusterer import HDBSCANClusterer\n",
    "import numpy as np\n",
    "\n",
    "clusterer = HDBSCANClusterer(min_cluster_size=10, min_samples=3, use_gpu=True)\n",
    "labels = clusterer.cluster(reduced_two_stage)\n",
    "\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "n_clusters = len(unique) - (1 if -1 in unique else 0)\n",
    "n_noise = counts[unique == -1][0] if -1 in unique else 0\n",
    "\n",
    "print(f\"Found {n_clusters} clusters, with {n_noise} noise points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_reducer = UMAPReducer(n_components=2, use_gpu=True)\n",
    "points_2d = viz_reducer.reduce(reduced_two_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phrasely.evaluation.dbcv_score import compute_dbcv\n",
    "mask = labels != -1\n",
    "score = compute_dbcv(reduced_two_stage[mask], labels[mask])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# ðŸ§­ Phrasely Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 1. Summary Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "from phrasely.medoids.medoid_selector import MedoidSelector\n",
    "from phrasely.evaluation.dbcv_score import compute_dbcv\n",
    "\n",
    "mask = labels != -1\n",
    "X_valid, y_valid = reduced_two_stage[mask], labels[mask]\n",
    "\n",
    "silhouette = None\n",
    "try:\n",
    "    sample_idx = np.random.choice(len(X_valid), size=5000, replace=False)\n",
    "    silhouette = silhouette_score(X_valid[sample_idx], y_valid[sample_idx])\n",
    "except Exception as e:\n",
    "    print(f\"Silhouette skipped: {e}\")\n",
    "\n",
    "ch = calinski_harabasz_score(X_valid, y_valid)\n",
    "db = davies_bouldin_score(X_valid, y_valid)\n",
    "dbcv = compute_dbcv(X_valid, y_valid)\n",
    "\n",
    "print(f\"Silhouette: {silhouette:.3f}\" if silhouette else \"Silhouette: N/A\")\n",
    "print(f\"Calinskiâ€“Harabasz: {ch:.1f}\")\n",
    "print(f\"Daviesâ€“Bouldin: {db:.3f}\")\n",
    "print(f\"DBCV: {dbcv:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 2. UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(points_2d[:, 0], points_2d[:, 1], c=labels, s=2, cmap=\"Spectral\")\n",
    "plt.title(f\"HDBSCAN Clusters (DBCV={dbcv:.2f})\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 3. Cluster Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "cluster_sizes = counts[unique != -1]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(cluster_sizes, bins=60, color=\"gray\")\n",
    "plt.title(\"Cluster Size Distribution\")\n",
    "plt.xlabel(\"Cluster Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total clusters: {len(cluster_sizes)}\")\n",
    "print(f\"Noise points: {counts[unique == -1][0] if -1 in unique else 0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 4. Medoid Phrase Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selector = MedoidSelector()\n",
    "medoids = selector.select(phrases, reduced_two_stage, labels)\n",
    "\n",
    "print(\"Sample medoid phrases:\")\n",
    "for i, m in enumerate(medoids[:15]):\n",
    "    print(f\"{i:3d}: {m}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 5. Cohesion / Separation Diagnostics (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "mask = labels != -1\n",
    "X_sub, y_sub = X_valid[:5000], y_valid[:5000]  # sample for speed\n",
    "D = pairwise_distances(X_sub)\n",
    "\n",
    "intra = np.mean([D[y_sub == c][:, y_sub == c].mean() for c in np.unique(y_sub)])\n",
    "inter = np.mean([D[y_sub == c][:, y_sub != c].mean() for c in np.unique(y_sub)])\n",
    "print(f\"Intra-cluster mean distance: {intra:.3f}\")\n",
    "print(f\"Inter-cluster mean distance: {inter:.3f}\")\n",
    "print(f\"Separation ratio (inter/intra): {inter/intra:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "mask = labels != -1\n",
    "X, y = reduced_two_stage[mask], labels[mask]\n",
    "unique_labels = np.unique(y)\n",
    "\n",
    "cohesions, separations, sizes = [], [], []\n",
    "\n",
    "D = pairwise_distances(X)\n",
    "\n",
    "for c in unique_labels:\n",
    "    idx = np.where(y == c)[0]\n",
    "    if len(idx) < 5:\n",
    "        continue\n",
    "    intra = D[np.ix_(idx, idx)].mean()\n",
    "    others = np.where(y != c)[0]\n",
    "    inter = D[np.ix_(idx, others)].min()\n",
    "    cohesions.append(intra)\n",
    "    separations.append(inter)\n",
    "    sizes.append(len(idx))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sc = plt.scatter(cohesions, separations, s=np.sqrt(sizes), alpha=0.6, cmap='viridis')\n",
    "plt.xlabel(\"Intra-cluster distance (Cohesion â†“)\")\n",
    "plt.ylabel(\"Nearest inter-cluster distance (Separation â†‘)\")\n",
    "plt.title(\"Cluster Cohesion vs Separation\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"phrase\": phrases_sample,\n",
    "    \"label\": labels\n",
    "})\n",
    "df.to_parquet(\"data/cluster_results.parquet\", index=False)\n",
    "print(\"âœ… Saved cluster results to cluster_results.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phrasely (GPU)",
   "language": "python",
   "name": "phrasely-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
