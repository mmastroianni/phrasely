{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd19825e-2793-4661-a087-5d0907d9abca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e13413-558d-44fd-b664-51e487a98c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/micromamba/envs/phrasely-gpu/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"0\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/tmp\"     # points to temp dir\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"HF_DATASETS_DISABLE_CACHING\"] = \"1\"  # ðŸ‘ˆ full off switch\n",
    "\n",
    "project_root = \"/home/michael/workspace/phrasely\"\n",
    "sys.path.insert(0, os.path.join(project_root, \"src\"))\n",
    "os.chdir(project_root)\n",
    "\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "from tqdm import TqdmWarning, tqdm\n",
    "import pandas as pd\n",
    "from phrasely.pipeline import run_pipeline\n",
    "from phrasely.data_loading.cc100_loader import CC100Loader\n",
    "from phrasely.embeddings.phrase_embedder import PhraseEmbedder\n",
    "from phrasely.data_loading.cc100_offline_loader import CC100OfflineLoader\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=TqdmWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d3a43-d608-4392-bfda-2f99dae12e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "915aa3a5-f7ed-4e12-b499-21a24b49a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(name: str) -> logging.Logger:\n",
    "    logger = logging.getLogger(name)\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        formatter = logging.Formatter(\"%(message)s\")\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.setLevel(logging.INFO)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc744a1b-e2d0-4c17-a92d-652938032c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger phrasely (INFO)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_logger('phrasely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6357a-2220-43e4-9d08-36dd096ed8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22e94f6-715d-46f4-bb1f-078c210a42ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Phrasely pipeline...\n",
      "Detected GPU VRAM: 3.8 GB\n",
      "Adaptive GPU limits â€” SVD: 190,000 rows, HDBSCAN: 190,000 rows.\n",
      "â–¶ï¸  Loading phrases...\n",
      "Resuming from 20 existing chunks in data_cache/cc100_phrases_en_parts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering CC100 shards: 1000000it [00:00, 25420024242.42it/s]                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and saved 1,000,000 phrases.\n",
      "Merging parts from data_cache/cc100_phrases_en_parts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 1,500,000 phrases into data_cache/cc100_phrases_en.parquet\n",
      "Loaded 1,500,000 CC100 offline phrases.\n",
      "Loading phrases completed in 1.377s.\n",
      "â–¶ï¸  Embedding phrases...\n",
      "PhraseEmbedder using model=paraphrase-MiniLM-L6-v2, device=cuda, VRAMâ‰ˆ3.8 GB, batch_size=8\n",
      "Converted model to fp16 for reduced VRAM usage.\n",
      "Loading cached embeddings from data_cache/embeddings_paraphrase-MiniLM-L6-v2_4f64f06e08fffd13fd3698dd29470c07.npy\n",
      "ðŸ§¹ Freed GPU memory after embedding.\n",
      "Embedding phrases completed in 1.122s.\n",
      "â–¶ï¸  Reducing dimensions...\n",
      "SVDReducer: using GPU backend for TruncatedSVD.\n",
      "SVDReducer: reduced 384 â†’ 100dimensions.\n",
      "ðŸ§¹ Freed GPU memory after SVD reduction.\n",
      "Reducing dimensions completed in 0.488s.\n",
      "â–¶ï¸  Clustering phrases...\n",
      "HDBSCANClusterer: using GPU backend.\n",
      "HDBSCANClusterer: found 41 clusters (+ noise).\n",
      "ðŸ§¹ Freed GPU memory after clustering.\n",
      "Clustering phrases completed in 14.006s.\n",
      "â–¶ï¸  Selecting medoids...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MedoidSelector.select() missing 1 required positional argument: 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# --- keep this small just to verify everything completes cleanly ---\u001b[39;00m\n\u001b[1;32m      5\u001b[0m loader_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrow_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_cache/cc100\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_phrases\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100_000\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50_000\u001b[39m,\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 11\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCC100OfflineLoader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# save the results for later exploration\u001b[39;00m\n\u001b[1;32m     19\u001b[0m result\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_cache/run_cc100_100k\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/phrasely/src/phrasely/pipeline.py:96\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(loader_cls, loader_kwargs, n_components, use_gpu, min_cluster_size, min_samples)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m catch_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelecting medoids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     95\u001b[0m     selector \u001b[38;5;241m=\u001b[39m MedoidSelector()\n\u001b[0;32m---> 96\u001b[0m     medoids \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# --- Results ---\u001b[39;00m\n\u001b[1;32m     99\u001b[0m n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(labels)) \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m labels \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: MedoidSelector.select() missing 1 required positional argument: 'labels'"
     ]
    }
   ],
   "source": [
    "from phrasely.pipeline import run_pipeline\n",
    "from phrasely.data_loading.cc100_offline_loader import CC100OfflineLoader\n",
    "\n",
    "# --- keep this small just to verify everything completes cleanly ---\n",
    "loader_args = {\n",
    "    \"arrow_dir\": \"data_cache/cc100\",\n",
    "    \"max_phrases\": 100_000,\n",
    "    \"chunk_size\": 50_000,\n",
    "}\n",
    "\n",
    "result = run_pipeline(\n",
    "    CC100OfflineLoader,\n",
    "    loader_kwargs=loader_args,\n",
    "    n_components=100,\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "# save the results for later exploration\n",
    "result.save(\"data_cache/run_cc100_100k\")\n",
    "\n",
    "print(\"\\nâœ… End-to-end pipeline finished successfully!\")\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0d272-70bd-4243-8744-8eb64aa23f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d9a40-28bf-4324-891d-dd5c47d11000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phrasely (GPU)",
   "language": "python",
   "name": "phrasely-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
